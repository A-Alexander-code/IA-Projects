{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/shankhad/v2-mtl-li-claim-sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Language Detection, Claim Identification, and Sentiment Analysis Project\n",
    "In today's globalized world, the ability to understand and process multiple languages is more important than ever. This project aims to develop a comprehensive Natural Language Processing (NLP) model that not only detects languages but also identifies claims and analyzes sentiment in textual data. Leveraging a diverse dataset comprising 17 languages, this project seeks to advance the capabilities of multilingual text processing.\n",
    "\n",
    "## Background on Natural Language Processing (NLP)\n",
    "\n",
    "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. By enabling machines to understand, interpret, and generate human language, NLP bridges the gap between human communication and computer understanding. The evolution of NLP has seen significant milestones, from early rule-based systems to the more advanced machine learning and deep learning models used today.<br>\n",
    "\n",
    "Key techniques in NLP include tokenization, part-of-speech tagging, named entity recognition, parsing, and sentiment analysis. Modern advancements, particularly with neural networks and transformer-based models like BERT and GPT, have significantly improved the accuracy and capabilities of NLP applications. These advancements enable more nuanced language understanding and generation, making it possible for NLP to be applied in a variety of fields such as customer service, healthcare, finance, and more.<br>\n",
    "\n",
    "In the context of this project, NLP techniques will be utilized to detect languages, identify claims, and analyze sentiment across texts in 17 different languages. By leveraging the latest advancements in NLP, we aim to build robust models that can handle the complexity and diversity of multilingual text processing.<br>\n",
    "\n",
    "## About the Dataset\n",
    "\n",
    "The dataset utilized in this project is a compact yet diverse language detection dataset. It encompasses text details from the following 17 languages:\n",
    "1. English\n",
    "2. Malayalam\n",
    "3. Hindi\n",
    "4. Tamil\n",
    "5. Kannada\n",
    "6. French\n",
    "7. Spanish\n",
    "8. Portuguese\n",
    "9. Italian\n",
    "10. Russian\n",
    "11. Swedish\n",
    "12. Dutch\n",
    "13. Arabic\n",
    "14. Turkish\n",
    "15. German\n",
    "16. Danish\n",
    "17. Greek\n",
    "### Source\n",
    "[Claim detection](https://www.kaggle.com/datasets/shankhad/claim-detection-all)<br>\n",
    "[Language detection](https://www.kaggle.com/datasets/basilb2s/language-detection/data)<br>\n",
    "[Language Identification dataset](https://www.kaggle.com/datasets/zarajamshaid/language-identification-datasst)<br>\n",
    "[Sentiment analysis](https://www.kaggle.com/datasets/shankhad/sentiment-analysis-all)<br>\n",
    "[Glove300](https://www.kaggle.com/datasets/shankhad/glove300)\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Language Detection: Build an NLP model capable of accurately identifying the language of a given text from the 17 provided languages.\n",
    "- Claim Identification: Develop algorithms to detect and extract claims or statements from the text, regardless of the language.\n",
    "- Sentiment Analysis: Implement sentiment analysis tools to determine the emotional tone or sentiment (positive, negative, or neutral) of the text in various languages.\n",
    "\n",
    "## Significance\n",
    "\n",
    "This project holds significant value for various applications such as social media monitoring, customer feedback analysis, and content moderation, where understanding multiple languages and deriving insights from text is crucial. By achieving these objectives, we aim to contribute to the broader field of NLP and enhance the tools available for multilingual text analysis.\n",
    "\n",
    "## Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "style.use('ggplot')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, GRU, SimpleRNN, Embedding, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy, CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.layers import concatenate\n",
    "from keras import callbacks\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "### Data Path Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claim=pd.read_csv('Data\\claim-detection-all.csv')\n",
    "df_sentiment=pd.read_csv('Data\\sentiment_analysis-all.csv')\n",
    "df_lang_1=pd.read_csv('Data\\dataset.csv')\n",
    "df_lang_2=pd.read_csv('Data\\Language Detection.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame `df_claim` with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>claim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Also , thinned out superfluous links .</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>As to the two points , Guttmacher himself died...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Cybercast New Service is not a neutral source .</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I am satisfied with Severa 's proposed comprom...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I have moved it too , but I will not move it b...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text claim\n",
       "0           0             Also , thinned out superfluous links .     N\n",
       "1           1  As to the two points , Guttmacher himself died...     Y\n",
       "2           2    Cybercast New Service is not a neutral source .     Y\n",
       "3           3  I am satisfied with Severa 's proposed comprom...     Y\n",
       "4           4  I have moved it too , but I will not move it b...     N"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_claim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A series of transformations will be applied to the DataFrame `df_claim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claim=df_claim.drop(['Unnamed: 0'], axis=1)\n",
    "claim2id={'N': 0, 'Y':1}\n",
    "df_claim['claim']=df_claim['claim'].apply(lambda x: claim2id[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>claim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3585</th>\n",
       "      <td>Sax , on the other hand , is kind of a socio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <td>Taking on the Panther-ness , representing the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7771</th>\n",
       "      <td>@sardesairajdeep Thank god. At least he knows ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7092</th>\n",
       "      <td>RT @Stunneri: abeg let someone bring a cure fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11343</th>\n",
       "      <td>The Woodward book has already been refuted and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  claim\n",
       "3585   Sax , on the other hand , is kind of a socio...      1\n",
       "2701   Taking on the Panther-ness , representing the ...      1\n",
       "7771   @sardesairajdeep Thank god. At least he knows ...      1\n",
       "7092   RT @Stunneri: abeg let someone bring a cure fo...      0\n",
       "11343  The Woodward book has already been refuted and...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into training and test sets\n",
    "seed=42\n",
    "df_train_claim, df_test_claim=train_test_split(df_claim, test_size=0.2, random_state=seed)\n",
    "df_train_claim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>claim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>It would seem the reference is an excellent th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6070</th>\n",
       "      <td>@evankirstel Or was (disappeared) the lead CCP...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6596</th>\n",
       "      <td>should be trusted to manage the coronavirusout...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>Correction: The MLK bust is still in the Oval ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11119</th>\n",
       "      <td>Wow. So it seems covering yourself in #JesusBl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  claim\n",
       "251    It would seem the reference is an excellent th...      1\n",
       "6070   @evankirstel Or was (disappeared) the lead CCP...      1\n",
       "6596   should be trusted to manage the coronavirusout...      0\n",
       "5426   Correction: The MLK bust is still in the Oval ...      1\n",
       "11119  Wow. So it seems covering yourself in #JesusBl...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_claim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame `df_sentiment` with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text sentiment\n",
       "0           0                I`d have responded, if I were going   neutral\n",
       "1           1      Sooo SAD I will miss you here in San Diego!!!  negative\n",
       "2           2                          my boss is bullying me...  negative\n",
       "3           3                     what interview! leave me alone  negative\n",
       "4           4   Sons of ****, why couldn`t they put them on t...  negative"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A series of transformations will be applied to the DataFrame `df_sentiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaN values\n",
    "df_sentiment=df_sentiment.dropna()\n",
    "\n",
    "df_sentiment=df_sentiment.drop(['Unnamed: 0'], axis=1)\n",
    "sentiment2id={'positive':0, 'negative':1, 'neutral':2}\n",
    "df_sentiment['sentiment']=df_sentiment['sentiment'].apply(lambda x: sentiment2id[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12828</th>\n",
       "      <td>Building Websites</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16468</th>\n",
       "      <td>is still pretty depressed about losing her hel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5197</th>\n",
       "      <td>blahh i`m tired and i gotta go to the airport ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20204</th>\n",
       "      <td>Hehe! It is nice down there</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16920</th>\n",
       "      <td>won`t be going to the Oxford Internet Institut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "12828                                  Building Websites          2\n",
       "16468  is still pretty depressed about losing her hel...          1\n",
       "5197   blahh i`m tired and i gotta go to the airport ...          1\n",
       "20204                        Hehe! It is nice down there          0\n",
       "16920  won`t be going to the Oxford Internet Institut...          1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into training and test sets\n",
    "df_train_sentiment, df_test_sentiment=train_test_split(df_sentiment, train_size=len(df_train_claim), test_size=len(df_test_claim), random_state=seed)\n",
    "\n",
    "df_train_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22131</th>\n",
       "      <td>I was afraid you were going to say that.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15632</th>\n",
       "      <td>part 2: social networking??.. there is even r...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17622</th>\n",
       "      <td>i miss the one who would do anything to spend ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25332</th>\n",
       "      <td>tee we beefin....what was u supposed to do b...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26008</th>\n",
       "      <td>Happy Hug Your Mom Day!! love you mom</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "22131           I was afraid you were going to say that.          1\n",
       "15632   part 2: social networking??.. there is even r...          2\n",
       "17622  i miss the one who would do anything to spend ...          1\n",
       "25332    tee we beefin....what was u supposed to do b...          2\n",
       "26008              Happy Hug Your Mom Day!! love you mom          0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A series of transformations will be applied to the DataFrames `df_lang_1` and `df_lang_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lang_2=df_lang_2.rename(columns={'Language':'language'})\n",
    "df_lang=pd.concat([df_lang_1, df_lang_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "df_train_lang, df_test_lang=train_test_split(df_lang, train_size=len(df_train_claim), test_size=len(df_test_claim), random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment uses a `LabelEncoder` to transform categorical labels in a DataFrame column into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 24, 27, 26,  3, 12, 28, 15, 29, 10, 19,  6,  1, 14,  9, 23, 20,\n",
       "       17, 21, 22,  4,  0, 16, 18,  8,  2, 11, 25,  7, 13])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le=LabelEncoder()\n",
    "df_lang['language']=le.fit_transform(df_lang['language'])\n",
    "df_lang['language'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment uses a `LabelEncoder` to decode numerical values back into their original categorical labels and adds the decoded labels to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "      <th>decoded_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Estonian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
       "      <td>24</td>\n",
       "      <td>Swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
       "      <td>27</td>\n",
       "      <td>Thai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
       "      <td>26</td>\n",
       "      <td>Tamil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
       "      <td>3</td>\n",
       "      <td>Dutch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32332</th>\n",
       "      <td>ನಿಮ್ಮ ತಪ್ಪು ಏನು ಬಂದಿದೆಯೆಂದರೆ ಆ ದಿನದಿಂದ ನಿಮಗೆ ಒ...</td>\n",
       "      <td>13</td>\n",
       "      <td>Kannada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32333</th>\n",
       "      <td>ನಾರ್ಸಿಸಾ ತಾನು ಮೊದಲಿಗೆ ಹೆಣಗಾಡುತ್ತಿದ್ದ ಮಾರ್ಗಗಳನ್...</td>\n",
       "      <td>13</td>\n",
       "      <td>Kannada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32334</th>\n",
       "      <td>ಹೇಗೆ ' ನಾರ್ಸಿಸಿಸಮ್ ಈಗ ಮರಿಯನ್ ಅವರಿಗೆ ಸಂಭವಿಸಿದ ಎ...</td>\n",
       "      <td>13</td>\n",
       "      <td>Kannada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32335</th>\n",
       "      <td>ಅವಳು ಈಗ ಹೆಚ್ಚು ಚಿನ್ನದ ಬ್ರೆಡ್ ಬಯಸುವುದಿಲ್ಲ ಎಂದು ...</td>\n",
       "      <td>13</td>\n",
       "      <td>Kannada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32336</th>\n",
       "      <td>ಟೆರ್ರಿ ನೀವು ನಿಜವಾಗಿಯೂ ಆ ದೇವದೂತನಂತೆ ಸ್ವಲ್ಪ ಕಾಣು...</td>\n",
       "      <td>13</td>\n",
       "      <td>Kannada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32337 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  language  \\\n",
       "0      klement gottwaldi surnukeha palsameeriti ning ...         5   \n",
       "1      sebes joseph pereira thomas  på eng the jesuit...        24   \n",
       "2      ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...        27   \n",
       "3      விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...        26   \n",
       "4      de spons behoort tot het geslacht haliclona en...         3   \n",
       "...                                                  ...       ...   \n",
       "32332  ನಿಮ್ಮ ತಪ್ಪು ಏನು ಬಂದಿದೆಯೆಂದರೆ ಆ ದಿನದಿಂದ ನಿಮಗೆ ಒ...        13   \n",
       "32333  ನಾರ್ಸಿಸಾ ತಾನು ಮೊದಲಿಗೆ ಹೆಣಗಾಡುತ್ತಿದ್ದ ಮಾರ್ಗಗಳನ್...        13   \n",
       "32334  ಹೇಗೆ ' ನಾರ್ಸಿಸಿಸಮ್ ಈಗ ಮರಿಯನ್ ಅವರಿಗೆ ಸಂಭವಿಸಿದ ಎ...        13   \n",
       "32335  ಅವಳು ಈಗ ಹೆಚ್ಚು ಚಿನ್ನದ ಬ್ರೆಡ್ ಬಯಸುವುದಿಲ್ಲ ಎಂದು ...        13   \n",
       "32336  ಟೆರ್ರಿ ನೀವು ನಿಜವಾಗಿಯೂ ಆ ದೇವದೂತನಂತೆ ಸ್ವಲ್ಪ ಕಾಣು...        13   \n",
       "\n",
       "      decoded_language  \n",
       "0             Estonian  \n",
       "1              Swedish  \n",
       "2                 Thai  \n",
       "3                Tamil  \n",
       "4                Dutch  \n",
       "...                ...  \n",
       "32332          Kannada  \n",
       "32333          Kannada  \n",
       "32334          Kannada  \n",
       "32335          Kannada  \n",
       "32336          Kannada  \n",
       "\n",
       "[32337 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_languages=le.inverse_transform(df_lang['language'])\n",
    "df_lang['decoded_language']=decoded_languages\n",
    "df_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Unique Decoded Languages (Ascending Order):\n",
      "Language: Arabic - Code: 0\n",
      "Language: Chinese - Code: 1\n",
      "Language: Danish - Code: 2\n",
      "Language: Dutch - Code: 3\n",
      "Language: English - Code: 4\n",
      "Language: Estonian - Code: 5\n",
      "Language: French - Code: 6\n",
      "Language: German - Code: 7\n",
      "Language: Greek - Code: 8\n",
      "Language: Hindi - Code: 9\n",
      "Language: Indonesian - Code: 10\n",
      "Language: Italian - Code: 11\n",
      "Language: Japanese - Code: 12\n",
      "Language: Kannada - Code: 13\n",
      "Language: Korean - Code: 14\n",
      "Language: Latin - Code: 15\n",
      "Language: Malayalam - Code: 16\n",
      "Language: Persian - Code: 17\n",
      "Language: Portugeese - Code: 18\n",
      "Language: Portugese - Code: 19\n",
      "Language: Pushto - Code: 20\n",
      "Language: Romanian - Code: 21\n",
      "Language: Russian - Code: 22\n",
      "Language: Spanish - Code: 23\n",
      "Language: Swedish - Code: 24\n",
      "Language: Sweedish - Code: 25\n",
      "Language: Tamil - Code: 26\n",
      "Language: Thai - Code: 27\n",
      "Language: Turkish - Code: 28\n",
      "Language: Urdu - Code: 29\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "unique_decoded_languages=sorted(df_lang['decoded_language'].unique())\n",
    "unique_language=sorted(df_lang['language'].unique())\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Unique Decoded Languages (Ascending Order):\")\n",
    "for i in range(len(unique_decoded_languages)):\n",
    "    print(f\"Language: {unique_decoded_languages[i]} - Code: {i}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment uses a `LabelEncoder` to transform categorical labels into numerical values for both training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_lang['language']=le.transform(df_train_lang['language'])\n",
    "df_test_lang['language']=le.transform(df_test_lang['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17739</th>\n",
       "      <td>گلخانه باغ ایرانی متعلق به مجوعه سعدآباد بوده ...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21311</th>\n",
       "      <td>இலக்கியம் ஒரு முழுமையான நுண்கலை அதற்கென ஒரு தன...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14851</th>\n",
       "      <td>तेय़ार कर रहें हैं। उन के काम की सब से बड़ी मु...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26618</th>\n",
       "      <td>wees voorzichtig.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17688</th>\n",
       "      <td>그는 일본의 조선 병합은 일본의 영토 팽창 욕구만이 있는게 아니라 일본이 아시아를 ...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  language\n",
       "17739  گلخانه باغ ایرانی متعلق به مجوعه سعدآباد بوده ...        17\n",
       "21311  இலக்கியம் ஒரு முழுமையான நுண்கலை அதற்கென ஒரு தன...        26\n",
       "14851  तेय़ार कर रहें हैं। उन के काम की सब से बड़ी मु...         9\n",
       "26618                                  wees voorzichtig.         3\n",
       "17688  그는 일본의 조선 병합은 일본의 영토 팽창 욕구만이 있는게 아니라 일본이 아시아를 ...        14"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_lang.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16151</th>\n",
       "      <td>मस्तिष्क ज्वर के दौरान बड़े स्तर पर सबअर्कनॉएड...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19058</th>\n",
       "      <td>اوبه دځمکې هوا تنظيموي که اوبه نه وي نودهوا تو...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30688</th>\n",
       "      <td>om du vill tacka någon kan du säga att det är ...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23314</th>\n",
       "      <td>thanking people.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29228</th>\n",
       "      <td>molto più semplice dell'HTML) il software Medi...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  language\n",
       "16151  मस्तिष्क ज्वर के दौरान बड़े स्तर पर सबअर्कनॉएड...         9\n",
       "19058  اوبه دځمکې هوا تنظيموي که اوبه نه وي نودهوا تو...        20\n",
       "30688  om du vill tacka någon kan du säga att det är ...        25\n",
       "23314                                   thanking people.         4\n",
       "29228  molto più semplice dell'HTML) il software Medi...        11"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_lang.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert sentences to sequences\n",
    "This code converts text data from different datasets (claim, sentiment, and language) into numerical sequences using a Tokenizer. This numerical representation is typically used as input for machine learning models.<br><br>\n",
    "__1. Initialization:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the maximum size of the vocabulary to 20,000 words\n",
    "MAX_VOCAB_SIZE=20000\n",
    "# Specifies whether the tokenizer should convert all characters to lowercase. In this case, it will not\n",
    "should_lowercase=False\n",
    "\n",
    "# Creates an instance of the Tokenizer \n",
    "tokenizer=Tokenizer(num_words=MAX_VOCAB_SIZE,\n",
    "                    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', # Defines characters to filter out from the text.\n",
    "                    lower=should_lowercase,\n",
    "                    oov_token='UNK' # Specifies a token for out-of-vocabulary words, labeled as \"UNK\" (Unknown)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Fit and transform text:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits the tokenizer on the 'text' column of the df_train_claim DataFrame, creating a word index based on the training claim texts\n",
    "tokenizer.fit_on_texts(df_train_claim['text'])\n",
    "# Converts the claim training texts into sequences of integers based on the fitted tokenizer\n",
    "sequences_train_claim=tokenizer.texts_to_sequences(df_train_claim['text'])\n",
    "# Converts the claim test texts into sequences of integers based on the fitted tokenizer\n",
    "sequences_test_claim=tokenizer.texts_to_sequences(df_test_claim['text'])\n",
    "\n",
    "# Fits the tokenizer on the 'text' column of the df_train_sentiment DataFrame, creating a word index based on the training sentiment texts\n",
    "tokenizer.fit_on_texts(df_train_sentiment['text'])\n",
    "# Converts the sentiment training texts into sequences of integers based on the fitted tokenizer\n",
    "sequences_train_sentiment=tokenizer.texts_to_sequences(df_train_sentiment['text'])\n",
    "# Converts the sentiment test texts into sequences of integers based on the fitted tokenizer\n",
    "sequence_test_sentiment=tokenizer.texts_to_sequences(df_test_sentiment['text'])\n",
    "\n",
    "# Fits the tokenizer on the 'Text' column of the df_train_lang DataFrame, creating a word index based on the training language texts\n",
    "tokenizer.fit_on_texts(df_train_lang['Text'])\n",
    "# Converts the language training texts into sequences of integers based on the fitted tokenizer\n",
    "sequences_train_lang=tokenizer.texts_to_sequences(df_train_lang['Text'])\n",
    "# Converts the language test texts into sequences of integers based on the fitted tokenizer\n",
    "sequences_test_lang=tokenizer.texts_to_sequences(df_test_lang['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code calculates and displays the length of the vocabulary used by the `Tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 157577\n"
     ]
    }
   ],
   "source": [
    "vocab_length=len(tokenizer.word_index)+1\n",
    "print(f\"Vocabulary length: {vocab_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment processes sequences of text data by padding them to a uniform length and then calculates and displays the lengths of the padded sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=300 # Sets the maximum length (maxlen) of the padded sequences to 300\n",
    "\n",
    "X_train_claim=pad_sequences(sequences_train_claim, padding='post', maxlen=D)\n",
    "X_test_claim=pad_sequences(sequences_test_claim, padding='post', maxlen=D)\n",
    "\n",
    "X_train_sentiment=pad_sequences(sequences_train_sentiment, padding='post', maxlen=D)\n",
    "X_test_sentiment=pad_sequences(sequence_test_sentiment, padding='post', maxlen=D)\n",
    "\n",
    "X_train_lang=pad_sequences(sequences_train_lang, padding='post', maxlen=D)\n",
    "X_test_lang=pad_sequences(sequences_test_lang, padding='post', maxlen=D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a tuple T containing the lengths of the padded sequences for the training data of claims, sentiment, and language. Each element of the tuple is the second dimension (shape[1]) of the respective padded sequences array, which should all be 300 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300, 300)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T=(X_train_claim.shape[1], X_train_sentiment.shape[1], X_train_lang.shape[1])\n",
    "\n",
    "# Showing the lengths of the padded sequences for each dataset\n",
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "This code segment loads GloVe word embeddings from a file and creates an embeddings dictionary where each word is mapped to its corresponding vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close()>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dictionary=dict()\n",
    "glove_file=open('Data\\glove.6B.300d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records=line.split()\n",
    "    word=records[0] # Extracts the word from the first component of the split line\n",
    "    vector_dimensions=asarray(records[1:], dtype='float32') # Converts the remaining components (vector dimensions) into a NumPy array of type float32\n",
    "    embeddings_dictionary[word]=vector_dimensions\n",
    "\n",
    "glove_file.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code segment creates an embedding matrix for a vocabulary using pre-trained GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix=zeros((vocab_length, 300))\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector=embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index]=embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157577, 300)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model\n",
    "This code segment defines a multi-input, multi-output neural network using Keras. It processes three different input sequences and makes predictions for three different tasks: claim classification, sentiment analysis, and language identification.<br><br>\n",
    "__Define Inputs:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=Input(shape=(300,)) # Defines an input tensor i with shape (300,)\n",
    "j=Input(shape=(300,)) # Defines an input tensor j with shape (300,)\n",
    "k=Input(shape=(300,)) # Defines an input tensor k with shape (300,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Combine inputs__<br><br>\n",
    "Concatenates the three input tensors along the last axis to form a single combined tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=concatenate([i, j, k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Embedding Layer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=Embedding(vocab_length, 300, weights=[embedding_matrix], input_length=T, trainable=False)(combined)\n",
    "x=Bidirectional(LSTM(512, return_sequences=True, activation='tanh'))(x)\n",
    "x=Dropout(0.5)(x)\n",
    "x=Bidirectional(LSTM(64, return_sequences=True, activation='tanh'))(x)\n",
    "x=GlobalMaxPooling1D()(x) # Applies global max pooling to reduce the dimensionality of the sequence outputs\n",
    "\n",
    "# Adds a dense layer with 64 units and 'tanh' activation for each classification\n",
    "x_claim_1=Dense(64, activation='tanh')(x)\n",
    "x_sentiment_1=Dense(64, activation='tanh')(x)\n",
    "x_lang_1=Dense(64, activation='tanh')(x)\n",
    "\n",
    "# Adds a dropout layer with 50% dropout rate to each classification path\n",
    "x_claim_2=Dropout(0.5)(x_claim_1)\n",
    "x_sentiment_2=Dropout(0.5)(x_sentiment_1)\n",
    "x_lang_2=Dropout(0.5)(x_lang_1)\n",
    "\n",
    "# Concatenates the dropout layers of sentiment analysis and claim classification paths\n",
    "combined1=concatenate([x_sentiment_2, x_claim_2])\n",
    "# Concatenates the dense layers of sentiment analysis, claim classification, and the dropout layer of language identification paths\n",
    "combined2=concatenate([x_sentiment_1, x_claim_1, x_lang_2])\n",
    "\n",
    "# dds a dense layer with a sigmoid activation function for binary claim classification\n",
    "x_claim=Dense(1, activation='sigmoid', name='claim')(x_claim_2)\n",
    "# Adds a dense layer with a softmax activation function for sentiment analysis with 3 classes\n",
    "x_sentiment=Dense(3, activation='softmax', name='sentiment')(combined1)\n",
    "# Adds a dense layer with a softmax activation function for language identification with a number of classes equal to the number of unique languages in the df_lang DataFrame\n",
    "x_lang=Dense(len(df_lang['language'].unique()), activation='softmax', name='lang')(combined2)\n",
    "\n",
    "# Create model\n",
    "model=Model(inputs=[i, j, k], outputs=[x_claim, x_sentiment, x_lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">900</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">900</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">47,273,100</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">900</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,330,048</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">900</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">900</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">557,568</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                     │                   │            │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ claim (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sentiment (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lang (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,790</span> │ concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m900\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m900\u001b[0m, \u001b[38;5;34m300\u001b[0m)  │ \u001b[38;5;34m47,273,100\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m900\u001b[0m, \u001b[38;5;34m1024\u001b[0m) │  \u001b[38;5;34m3,330,048\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m900\u001b[0m, \u001b[38;5;34m1024\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m900\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m557,568\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bidirectional_1[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│                     │                   │            │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ claim (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sentiment (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │        \u001b[38;5;34m387\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lang (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)        │      \u001b[38;5;34m5,790\u001b[0m │ concatenate_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,191,726</span> (195.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m51,191,726\u001b[0m (195.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,918,626</span> (14.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,918,626\u001b[0m (14.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">47,273,100</span> (180.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m47,273,100\u001b[0m (180.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and fit\n",
    "This code segment compiles and fits a multi-output neural network model using the Adam optimizer and specific loss functions for each output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.00005\n",
    "opt=Adam(learning_rate)\n",
    "#opt=RMSprop(learning_rate)\n",
    "#opt=SGD(learning_rate)\n",
    "\n",
    "model.compile(loss=[BinaryCrossentropy(from_logits=True),\n",
    "                    SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    SparseCategoricalCrossentropy(from_logits=True)],\n",
    "              optimizer=opt,\n",
    "              metrics=['acc']\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up an early stopping mechanism to halt training when the validation loss stops improving for 4 consecutive epochs, and it restores the model's weights to those from the epoch with the best validation loss. This technique helps in preventing overfitting and ensures that the best model is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping=callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=4, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code trains the neural network model on the training data for claims, sentiment, and language identification, using a batch size of 64 and running for up to 35 epochs. The validation data is used to monitor the model's performance, and early stopping is employed to halt training if the validation loss does not improve for 4 consecutive epochs, ensuring the best model weights are restored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=model.fit(\n",
    "    x=[X_train_claim, X_train_sentiment, X_train_lang],\n",
    "    y=[df_train_claim['claim'], df_train_sentiment['sentiment'], df_train_lang['language']],\n",
    "    epochs=35,\n",
    "    batch_size=64,\n",
    "    validation_data=([X_test_claim, X_test_sentiment, X_test_lang], \n",
    "                     [df_test_claim['claim'], df_test_sentiment['sentiment'], df_test_lang['language']]),\n",
    "    callbacks=[earlystopping]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
